---
title: "Exam 3"
author: "Antonio Tiu"
date: "7/9/2020"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. Clear environment
```{r}
rm(list=ls())
```

# 2 Using tidycensus
```{r}
library(tidycensus)

census_api_key("5e97ead132046e45d7523a868bab0829571d2f75")


#inequality_panel

```


# 3. Reshaping inequality_panel wide then using head to peak the data
```{r}
```


# 4. Reshape ineqaulity_wide to long format
```{r}
#long_data_frame <-
 # wide_by_year%>%
#  pivot_longer(cols =starts_with("year"),
#  names_prefix = "year_",
#  values_to = "current_amount",
#  values_drop_na = FALSE)%>%
#  filter(!(current_amount==0))
```


# 5. Showing that inequality_panel and inequality_long have the same data
```{r}

```


# 6. Collapse inequality_long data frame by state
```{r}
#collapsed_data <-
 # alldata%>%
 # group_by(year, state_name)%>%
 # summarize(across(where(is.numeric), sum))%>%
 # sumselect(-c("transaction_id"))
```


# 7. Produce map of the U.S that colors in the state polygons by their mean gini scores
```{r}

```


# 8. Use WDI package to import data 
```{r}

```


# 9. Deflate gdp_current to constant 2010 
```{r}
library(WDI)
deflator_data =WDI(country = "all", indicator =c("NY.GDP.DEFL.ZS"),
                   start = 2010,
                   end = 2010,
                   extra = FALSE, cache = NULL)
```


# 10. What are the three main components and their subcomponents?
The three main components of Shiny app are the "ui", "server", and "shinyApp". 

# 11. Pull .pdf file
```{r}
library(pdftools)
library(tidyverse)
library(dplyr)
library(purrr)
library(readr)

GDP <- pdf_text("usaid.pdf")%>%
  readr::read_lines()
```


# 12. Convert text pulled from the .pdf to a data frame
```{r}
library(tm)
#armeniatext <- function("usaid.pdf") {
 
#  dat = readPDF(control=list(text="-layout"))(elem=list(uri=file), 
                                     #         language="en", id="id1") 
#  dat = c(as.character(dat))

#  dat = dat[grep("^ {0,2}[0-9]{1,3}", dat)]

#  dat = gsub("^ ?([0-9]{1,3}) ?", "\\1|", dat)
#  dat = gsub("(, HVOL )","\\1 ", dat)
#  dat = gsub(" {2,100}", "|", dat)

#  excludeRows = lapply(gregexpr("\\|", dat), function(x) length(x)) != 6
#  write(dat[excludeRows], "rowsToCheck.txt", append=TRUE)

  
# dat = dat[!excludeRows]

 
#  dat = read.table(text=dat, sep="|", quote="", stringsAsFactors=FALSE)
 # names(dat) = c("RowNum", "Reference Entity", "Sub-Index", "CLIP", 
       #           "Reference Obligation", "CUSIP/ISIN", "Weighting")
#  return(dat)
#}

```


# 13. Tokenize the data by word and remove stop words
```{r}

# armeniatext=armeniatext%>%
# unnest_tokens(word, text)

#data(stop_words)

#armeniatext <- armeniatext%>%
 # anti_join(stop_words)

```

# 14. Figure out top 5 most used word 
```{r}
# frequency <- armeniatext%>%
 # count(word, sort = TRUE)

#frequency
```


# 15. Load Billboard Hot 100 webpage
```{r}
library(rvest)
library(dplyr)

hot100exam <- "https://www.billboard.com/charts/hot-100"
hot100 <- read_html(hot100exam)
hot100

```


# 16. Use rvest to obtain all of nodes in the webpage
```{r}
body_nodes <- hot100 %>%
  html_node("body") %>%
  html_children()
body_nodes
```



# 17. Using Google Chrome to identify the necessary tags 
```{r}

rank <- hot100 %>% 
  rvest::html_nodes('body') %>%
  xml2:: xml_find_all("//span[contains(@class, 
                      'chart-element__rank__number')]") %>%
  rvest::html_text()

artist <- hot100 %>% 
  rvest::html_nodes('body') %>%
  xml2:: xml_find_all("//span[contains(@class, 
                      'chart-element__information__artist')]") %>%
  rvest::html_text()
  
title <- hot100 %>% 
  rvest::html_nodes('body') %>%
  xml2:: xml_find_all("//span[contains(@class, 
                      'chart-element__information__song')]") %>%
  rvest::html_text()


chart_df <- data.frame(rank,artist,title)
```

# Link to Github repo
https://github.com/antoniotiu21/exam3


